{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMrrmmT3Y8kgn9ledOtiF62"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"cbhCRdKMyyUP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --upgrade datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BD_prsMKpK8Z","executionInfo":{"status":"ok","timestamp":1733825758496,"user_tz":-540,"elapsed":4584,"user":{"displayName":"전현준","userId":"12217885041222534855"}},"outputId":"2bd71ef9-d383-4123-dbab-dd442c231685"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Subset\n","from torchvision import transforms\n","from datasets import load_dataset\n","from transformers import CLIPModel, CLIPProcessor\n","import random\n","\n","# 1. EuroSAT 데이터셋 로드\n","dataset = load_dataset('Honaker/eurosat_dataset')\n","\n","# 라벨 인덱스를 클래스명으로 매핑\n","label2class = [\n","    \"Annual crop\",\n","    \"Forest\",\n","    \"Herbaceous vegetation\",\n","    \"Highway\",\n","    \"Industrial\",\n","    \"Pasture\",\n","    \"Permanent crop\",\n","    \"Residential\",\n","    \"River\",\n","    \"Sea/Lake\"\n","]\n","\n","# 이미지 변환 (CLIP 모델 표준 입력 크기: 224x224)\n","transform = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n","                         (0.26862954, 0.26130258, 0.27577711)) # CLIP mean/std\n","])\n","\n","def collate_fn(examples):\n","    images = [transform(x[\"image\"]) for x in examples]\n","    labels = [x[\"label\"] for x in examples]\n","    images = torch.stack(images)  # [batch, 3, 224, 224]\n","    labels = torch.tensor(labels) # [batch]\n","    return images, labels\n","\n","# 2. 훈련 데이터셋에서 80개 샘플 선택\n","train_dataset = dataset[\"train\"]\n","random.seed(42)  # 재현성을 위해 시드 고정\n","indices = random.sample(range(len(train_dataset)), 80)\n","subset_dataset = Subset(train_dataset, indices)\n","\n","# 3. DataLoader 생성\n","train_dataloader = DataLoader(subset_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n","\n","# 4. CLIP 모델 로드 및 freeze\n","model_name = \"openai/clip-vit-base-patch32\"\n","model = CLIPModel.from_pretrained(model_name)\n","processor = CLIPProcessor.from_pretrained(model_name)\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","model.eval()\n","\n","# CLIP 임베딩 차원 확인 (text & image 동일 차원)\n","embed_dim = model.config.projection_dim\n","\n","# 학습 대상 weight vector w 정의 (문장 임베딩 차원과 동일)\n","w = nn.Parameter(torch.zeros(embed_dim, dtype=torch.float32))\n","optimizer = optim.Adam([w], lr=1e-3)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","w = w.to(device)\n","\n","epochs = 100\n","for epoch in range(epochs):\n","    running_loss = 0.0\n","    for images, labels in train_dataloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        texts = [f\"A satellite image showing a {label2class[l].lower()}.\" for l in labels.tolist()]\n","        text_inputs = processor.tokenizer(\n","            texts, padding=True, truncation=True, return_tensors=\"pt\"\n","        ).to(device)\n","\n","        with torch.no_grad():\n","            # 이미지 임베딩 추출\n","            image_embeds = model.get_image_features(pixel_values=images)\n","            # 텍스트 임베딩 추출\n","            text_embeds = model.get_text_features(**text_inputs)\n","\n","        # w를 텍스트 임베딩에 더함 (텍스트 임베딩: [batch, embed_dim])\n","        # w: [embed_dim] 이므로 broadcast되어 각 배치 샘플 임베딩에 더해진다.\n","        sentence_embeds = text_embeds + w\n","\n","        # 노멀라이즈\n","        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n","        sentence_embeds = sentence_embeds / sentence_embeds.norm(p=2, dim=-1, keepdim=True)\n","\n","        logits = torch.matmul(image_embeds, sentence_embeds.t())\n","        target = torch.arange(logits.size(0), device=device)\n","        loss = (criterion(logits, target) + criterion(logits.t(), target)) / 2\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    avg_loss = running_loss / len(train_dataloader)\n","    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n","\n","    w_sum = w.sum().detach().cpu().item()\n","    print(\"w_sum scalar:\", w_sum)\n","    print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2V3rMXrCo-IJ","outputId":"ef6d5ce2-e0d0-407e-a66d-1db80644ca3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100, Loss: 1.3758\n","w_sum scalar: 0.0\n","\n","Epoch 2/100, Loss: 1.3754\n","w_sum scalar: 0.0\n","\n","Epoch 3/100, Loss: 1.3746\n","w_sum scalar: 0.0\n","\n","Epoch 4/100, Loss: 1.3755\n","w_sum scalar: 0.0\n","\n","Epoch 5/100, Loss: 1.3752\n","w_sum scalar: 0.0\n","\n","Epoch 6/100, Loss: 1.3767\n","w_sum scalar: 0.0\n","\n","Epoch 7/100, Loss: 1.3749\n","w_sum scalar: 0.0\n","\n","Epoch 8/100, Loss: 1.3757\n","w_sum scalar: 0.0\n","\n","Epoch 9/100, Loss: 1.3753\n","w_sum scalar: 0.0\n","\n","Epoch 10/100, Loss: 1.3774\n","w_sum scalar: 0.0\n","\n","Epoch 11/100, Loss: 1.3756\n","w_sum scalar: 0.0\n","\n","Epoch 12/100, Loss: 1.3762\n","w_sum scalar: 0.0\n","\n","Epoch 13/100, Loss: 1.3759\n","w_sum scalar: 0.0\n","\n","Epoch 14/100, Loss: 1.3753\n","w_sum scalar: 0.0\n","\n","Epoch 15/100, Loss: 1.3749\n","w_sum scalar: 0.0\n","\n","Epoch 16/100, Loss: 1.3759\n","w_sum scalar: 0.0\n","\n","Epoch 17/100, Loss: 1.3754\n","w_sum scalar: 0.0\n","\n","Epoch 18/100, Loss: 1.3750\n","w_sum scalar: 0.0\n","\n","Epoch 19/100, Loss: 1.3752\n","w_sum scalar: 0.0\n","\n","Epoch 20/100, Loss: 1.3752\n","w_sum scalar: 0.0\n","\n","Epoch 21/100, Loss: 1.3758\n","w_sum scalar: 0.0\n","\n","Epoch 22/100, Loss: 1.3753\n","w_sum scalar: 0.0\n","\n","Epoch 23/100, Loss: 1.3744\n","w_sum scalar: 0.0\n","\n","Epoch 24/100, Loss: 1.3749\n","w_sum scalar: 0.0\n","\n","Epoch 25/100, Loss: 1.3746\n","w_sum scalar: 0.0\n","\n","Epoch 26/100, Loss: 1.3749\n","w_sum scalar: 0.0\n","\n","Epoch 27/100, Loss: 1.3751\n","w_sum scalar: 0.0\n","\n","Epoch 28/100, Loss: 1.3752\n","w_sum scalar: 0.0\n","\n","Epoch 29/100, Loss: 1.3761\n","w_sum scalar: 0.0\n","\n","Epoch 30/100, Loss: 1.3753\n","w_sum scalar: 0.0\n","\n","Epoch 31/100, Loss: 1.3750\n","w_sum scalar: 0.0\n","\n","Epoch 32/100, Loss: 1.3744\n","w_sum scalar: 0.0\n","\n","Epoch 33/100, Loss: 1.3755\n","w_sum scalar: 0.0\n","\n","Epoch 34/100, Loss: 1.3754\n","w_sum scalar: 0.0\n","\n","Epoch 35/100, Loss: 1.3754\n","w_sum scalar: 0.0\n","\n","Epoch 36/100, Loss: 1.3755\n","w_sum scalar: 0.0\n","\n","Epoch 37/100, Loss: 1.3756\n","w_sum scalar: 0.0\n","\n","Epoch 38/100, Loss: 1.3753\n","w_sum scalar: 0.0\n","\n","Epoch 39/100, Loss: 1.3741\n","w_sum scalar: 0.0\n","\n","Epoch 40/100, Loss: 1.3757\n","w_sum scalar: 0.0\n","\n","Epoch 41/100, Loss: 1.3746\n","w_sum scalar: 0.0\n","\n","Epoch 42/100, Loss: 1.3744\n","w_sum scalar: 0.0\n","\n","Epoch 43/100, Loss: 1.3758\n","w_sum scalar: 0.0\n","\n","Epoch 44/100, Loss: 1.3748\n","w_sum scalar: 0.0\n","\n","Epoch 45/100, Loss: 1.3756\n","w_sum scalar: 0.0\n","\n","Epoch 46/100, Loss: 1.3753\n","w_sum scalar: 0.0\n","\n","Epoch 47/100, Loss: 1.3746\n","w_sum scalar: 0.0\n","\n","Epoch 48/100, Loss: 1.3752\n","w_sum scalar: 0.0\n","\n","Epoch 49/100, Loss: 1.3757\n","w_sum scalar: 0.0\n","\n","Epoch 50/100, Loss: 1.3745\n","w_sum scalar: 0.0\n","\n","Epoch 51/100, Loss: 1.3760\n","w_sum scalar: 0.0\n","\n","Epoch 52/100, Loss: 1.3756\n","w_sum scalar: 0.0\n","\n","Epoch 53/100, Loss: 1.3757\n","w_sum scalar: 0.0\n","\n","Epoch 54/100, Loss: 1.3745\n","w_sum scalar: 0.0\n","\n","Epoch 55/100, Loss: 1.3758\n","w_sum scalar: 0.0\n","\n","Epoch 56/100, Loss: 1.3754\n","w_sum scalar: 0.0\n","\n","Epoch 57/100, Loss: 1.3753\n","w_sum scalar: 0.0\n","\n","Epoch 58/100, Loss: 1.3744\n","w_sum scalar: 0.0\n","\n","Epoch 59/100, Loss: 1.3775\n","w_sum scalar: 0.0\n","\n","Epoch 60/100, Loss: 1.3759\n","w_sum scalar: 0.0\n","\n","Epoch 61/100, Loss: 1.3745\n","w_sum scalar: 0.0\n","\n","Epoch 62/100, Loss: 1.3751\n","w_sum scalar: 0.0\n","\n","Epoch 63/100, Loss: 1.3739\n","w_sum scalar: 0.0\n","\n","Epoch 64/100, Loss: 1.3751\n","w_sum scalar: 0.0\n","\n","Epoch 65/100, Loss: 1.3762\n","w_sum scalar: 0.0\n","\n","Epoch 66/100, Loss: 1.3756\n","w_sum scalar: 0.0\n","\n","Epoch 67/100, Loss: 1.3747\n","w_sum scalar: 0.0\n","\n","Epoch 68/100, Loss: 1.3746\n","w_sum scalar: 0.0\n","\n","Epoch 69/100, Loss: 1.3754\n","w_sum scalar: 0.0\n","\n","Epoch 70/100, Loss: 1.3748\n","w_sum scalar: 0.0\n","\n","Epoch 71/100, Loss: 1.3756\n","w_sum scalar: 0.0\n","\n","Epoch 72/100, Loss: 1.3755\n","w_sum scalar: 0.0\n","\n","Epoch 73/100, Loss: 1.3756\n","w_sum scalar: 0.0\n","\n","Epoch 74/100, Loss: 1.3757\n","w_sum scalar: 0.0\n","\n","Epoch 75/100, Loss: 1.3758\n","w_sum scalar: 0.0\n","\n","Epoch 76/100, Loss: 1.3752\n","w_sum scalar: 0.0\n","\n","Epoch 77/100, Loss: 1.3758\n","w_sum scalar: 0.0\n","\n","Epoch 78/100, Loss: 1.3753\n","w_sum scalar: 0.0\n","\n","Epoch 79/100, Loss: 1.3755\n","w_sum scalar: 0.0\n","\n","Epoch 80/100, Loss: 1.3751\n","w_sum scalar: 0.0\n","\n","Epoch 81/100, Loss: 1.3755\n","w_sum scalar: 0.0\n","\n","Epoch 82/100, Loss: 1.3753\n","w_sum scalar: 0.0\n","\n","Epoch 83/100, Loss: 1.3749\n","w_sum scalar: 0.0\n","\n","Epoch 84/100, Loss: 1.3747\n","w_sum scalar: 0.0\n","\n","Epoch 85/100, Loss: 1.3750\n","w_sum scalar: 0.0\n","\n","Epoch 86/100, Loss: 1.3752\n","w_sum scalar: 0.0\n","\n","Epoch 87/100, Loss: 1.3758\n","w_sum scalar: 0.0\n","\n","Epoch 88/100, Loss: 1.3760\n","w_sum scalar: 0.0\n","\n","Epoch 89/100, Loss: 1.3759\n","w_sum scalar: 0.0\n","\n","Epoch 90/100, Loss: 1.3755\n","w_sum scalar: 0.0\n","\n","Epoch 91/100, Loss: 1.3742\n","w_sum scalar: 0.0\n","\n","Epoch 92/100, Loss: 1.3749\n","w_sum scalar: 0.0\n","\n","Epoch 93/100, Loss: 1.3752\n","w_sum scalar: 0.0\n","\n","Epoch 94/100, Loss: 1.3747\n","w_sum scalar: 0.0\n","\n","Epoch 95/100, Loss: 1.3749\n","w_sum scalar: 0.0\n","\n","Epoch 96/100, Loss: 1.3755\n","w_sum scalar: 0.0\n","\n","Epoch 97/100, Loss: 1.3766\n","w_sum scalar: 0.0\n","\n","Epoch 98/100, Loss: 1.3754\n","w_sum scalar: 0.0\n","\n","Epoch 99/100, Loss: 1.3752\n","w_sum scalar: 0.0\n","\n","Epoch 100/100, Loss: 1.3747\n","w_sum scalar: 0.0\n","\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Subset\n","from torchvision import transforms\n","from datasets import load_dataset\n","from transformers import CLIPModel, CLIPProcessor\n","import random\n","\n","# EuroSAT 데이터셋 로드\n","dataset = load_dataset('Honaker/eurosat_dataset')\n","\n","label2class = [\n","    \"Annual crop\",\n","    \"Forest\",\n","    \"Herbaceous vegetation\",\n","    \"Highway\",\n","    \"Industrial\",\n","    \"Pasture\",\n","    \"Permanent crop\",\n","    \"Residential\",\n","    \"River\",\n","    \"Sea/Lake\"\n","]\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n","                         (0.26862954, 0.26130258, 0.27577711))\n","])\n","\n","def collate_fn(examples):\n","    images = [transform(x[\"image\"]) for x in examples]\n","    labels = [x[\"label\"] for x in examples]\n","    images = torch.stack(images)\n","    labels = torch.tensor(labels)\n","    return images, labels\n","\n","# 일부 샘플만 사용(예: 8개)\n","train_dataset = dataset[\"train\"]\n","random.seed(42)\n","indices = random.sample(range(len(train_dataset)), 8)\n","subset_dataset = Subset(train_dataset, indices)\n","\n","train_dataloader = DataLoader(subset_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n","\n","model_name = \"openai/clip-vit-base-patch32\"\n","model = CLIPModel.from_pretrained(model_name)\n","processor = CLIPProcessor.from_pretrained(model_name)\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","model.eval()\n","\n","embed_dim = model.config.projection_dim\n","w = nn.Parameter(torch.zeros(embed_dim, dtype=torch.float32))\n","\n","optimizer = optim.Adam([w], lr=1e-3)\n","criterion = nn.CrossEntropyLoss()\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","w = w.to(device)\n","\n","epochs = 3\n","\n","for epoch in range(epochs):\n","    running_loss = 0.0\n","    last_token_weight_sums = None\n","    last_texts = None\n","    last_input_ids = None\n","\n","    for images, labels in train_dataloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        texts = [f\"A satellite image showing a {label2class[l].lower()}.\" for l in labels.tolist()]\n","\n","        text_inputs = processor.tokenizer(\n","            texts, padding=True, truncation=True, return_tensors=\"pt\"\n","        ).to(device)\n","\n","        with torch.no_grad():\n","            image_embeds = model.get_image_features(pixel_values=images)\n","            text_outputs = model.text_model(**text_inputs)\n","            last_hidden_state = text_outputs.last_hidden_state  # [batch, seq_len, embed_dim]\n","\n","        # w 더하기\n","        last_hidden_state = last_hidden_state + w\n","\n","        # 토큰별 weight sum\n","        token_weight_sums = last_hidden_state.sum(dim=-1)  # [batch, seq_len]\n","\n","        # 문장 임베딩: 단순 평균 풀링 (예시)\n","        sentence_embeds = last_hidden_state.mean(dim=1)\n","\n","        # 정규화\n","        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n","        sentence_embeds = sentence_embeds / sentence_embeds.norm(p=2, dim=-1, keepdim=True)\n","\n","        logits = torch.matmul(image_embeds, sentence_embeds.t())\n","        target = torch.arange(logits.size(0), device=device)\n","        loss = (criterion(logits, target) + criterion(logits.t(), target)) / 2\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        last_token_weight_sums = token_weight_sums\n","        last_texts = texts\n","        last_input_ids = text_inputs[\"input_ids\"]\n","\n","    avg_loss = running_loss / len(train_dataloader)\n","    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n","\n","    # Epoch 종료 후 마지막 batch의 토큰별 weight sum 출력\n","    # 토큰 해석: 토크나이저의 decode 이용\n","    if last_token_weight_sums is not None:\n","        for i in range(last_token_weight_sums.size(0)):\n","            print(f\"Text {i+1}: {last_texts[i]}\")\n","            token_ids = last_input_ids[i].detach().cpu().tolist()\n","            tokens = processor.tokenizer.convert_ids_to_tokens(token_ids)\n","            # 패딩이나 스페셜 토큰 제거를 위해 실제 Attention Mask나 특수 토큰 처리 필요할 수도 있음.\n","            # 여기서는 단순히 토큰 시퀀스 전체를 출력\n","            print(\"Tokens:\", tokens)\n","            print(\"Token sums:\", last_token_weight_sums[i].detach().cpu().numpy())\n","            print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LeG3fKoYwwMS","executionInfo":{"status":"ok","timestamp":1733803842816,"user_tz":-540,"elapsed":3780,"user":{"displayName":"전현준","userId":"12217885041222534855"}},"outputId":"8d908cd9-d24e-41ce-a206-b5d2f59fef4e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Loss: 1.3851\n","Text 1: A satellite image showing a highway.\n","Tokens: ['<|startoftext|>', 'a</w>', 'satellite</w>', 'image</w>', 'showing</w>', 'a</w>', 'highway</w>', '.</w>', '<|endoftext|>', '<|endoftext|>']\n","Token sums: [54.16202  56.88982  60.224625 66.41615  63.714893 63.593544 63.531567\n"," 62.376896 62.487156 62.654423]\n","\n","Text 2: A satellite image showing a highway.\n","Tokens: ['<|startoftext|>', 'a</w>', 'satellite</w>', 'image</w>', 'showing</w>', 'a</w>', 'highway</w>', '.</w>', '<|endoftext|>', '<|endoftext|>']\n","Token sums: [54.16202  56.88982  60.224625 66.41615  63.714893 63.593544 63.531567\n"," 62.376896 62.487156 62.654423]\n","\n","Text 3: A satellite image showing a annual crop.\n","Tokens: ['<|startoftext|>', 'a</w>', 'satellite</w>', 'image</w>', 'showing</w>', 'a</w>', 'annual</w>', 'crop</w>', '.</w>', '<|endoftext|>']\n","Token sums: [54.16202  56.88982  60.224625 66.41615  63.714893 63.593544 62.018616\n"," 62.51574  61.727493 63.16903 ]\n","\n","Text 4: A satellite image showing a forest.\n","Tokens: ['<|startoftext|>', 'a</w>', 'satellite</w>', 'image</w>', 'showing</w>', 'a</w>', 'forest</w>', '.</w>', '<|endoftext|>', '<|endoftext|>']\n","Token sums: [54.16202  56.88982  60.224625 66.41615  63.714893 63.593544 63.791748\n"," 61.538334 62.33176  62.454098]\n","\n","Epoch 2/3, Loss: 1.3858\n","Text 1: A satellite image showing a forest.\n","Tokens: ['<|startoftext|>', 'a</w>', 'satellite</w>', 'image</w>', 'showing</w>', 'a</w>', 'forest</w>', '.</w>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n","Token sums: [54.16202  56.88982  60.224625 66.41615  63.714893 63.593544 63.791748\n"," 61.538334 62.33176  62.454098 62.55549 ]\n","\n","Text 2: A satellite image showing a sea/lake.\n","Tokens: ['<|startoftext|>', 'a</w>', 'satellite</w>', 'image</w>', 'showing</w>', 'a</w>', 'sea</w>', '/</w>', 'lake</w>', '.</w>', '<|endoftext|>']\n","Token sums: [54.16202  56.88982  60.224625 66.41615  63.714893 63.593544 61.63536\n"," 59.902565 59.19705  59.137867 62.334686]\n","\n","Text 3: A satellite image showing a forest.\n","Tokens: ['<|startoftext|>', 'a</w>', 'satellite</w>', 'image</w>', 'showing</w>', 'a</w>', 'forest</w>', '.</w>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n","Token sums: [54.16202  56.88982  60.224625 66.41615  63.714893 63.593544 63.791748\n"," 61.538334 62.33176  62.454098 62.55549 ]\n","\n","Text 4: A satellite image showing a forest.\n","Tokens: ['<|startoftext|>', 'a</w>', 'satellite</w>', 'image</w>', 'showing</w>', 'a</w>', 'forest</w>', '.</w>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n","Token sums: [54.16202  56.88982  60.224625 66.41615  63.714893 63.593544 63.791748\n"," 61.538334 62.33176  62.454098 62.55549 ]\n","\n","Epoch 3/3, Loss: 1.3845\n","Text 1: A satellite image showing a forest.\n","Tokens: ['<|startoftext|>', 'a</w>', 'satellite</w>', 'image</w>', 'showing</w>', 'a</w>', 'forest</w>', '.</w>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n","Token sums: [54.16202  56.88982  60.224625 66.41615  63.714893 63.593544 63.791748\n"," 61.538334 62.33176  62.454098 62.55549 ]\n","\n","Text 2: A satellite image showing a forest.\n","Tokens: ['<|startoftext|>', 'a</w>', 'satellite</w>', 'image</w>', 'showing</w>', 'a</w>', 'forest</w>', '.</w>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n","Token sums: [54.16202  56.88982  60.224625 66.41615  63.714893 63.593544 63.791748\n"," 61.538334 62.33176  62.454098 62.55549 ]\n","\n","Text 3: A satellite image showing a highway.\n","Tokens: ['<|startoftext|>', 'a</w>', 'satellite</w>', 'image</w>', 'showing</w>', 'a</w>', 'highway</w>', '.</w>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n","Token sums: [54.16202  56.88982  60.224625 66.41615  63.714893 63.593544 63.531567\n"," 62.376896 62.487156 62.654423 62.78783 ]\n","\n","Text 4: A satellite image showing a sea/lake.\n","Tokens: ['<|startoftext|>', 'a</w>', 'satellite</w>', 'image</w>', 'showing</w>', 'a</w>', 'sea</w>', '/</w>', 'lake</w>', '.</w>', '<|endoftext|>']\n","Token sums: [54.16202  56.88982  60.224625 66.41615  63.714893 63.593544 61.63536\n"," 59.902565 59.19705  59.137867 62.334686]\n","\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Subset\n","from torchvision import transforms\n","from datasets import load_dataset\n","from transformers import CLIPModel, CLIPProcessor\n","import random\n","\n","# EuroSAT 데이터셋 로드\n","dataset = load_dataset('Honaker/eurosat_dataset')\n","\n","label2class = [\n","    \"Annual crop\",\n","    \"Forest\",\n","    \"Herbaceous vegetation\",\n","    \"Highway\",\n","    \"Industrial\",\n","    \"Pasture\",\n","    \"Permanent crop\",\n","    \"Residential\",\n","    \"River\",\n","    \"Sea/Lake\"\n","]\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n","                         (0.26862954, 0.26130258, 0.27577711))\n","])\n","\n","def collate_fn(examples):\n","    images = [transform(x[\"image\"]) for x in examples]\n","    labels = [x[\"label\"] for x in examples]\n","    images = torch.stack(images)\n","    labels = torch.tensor(labels)\n","    return images, labels\n","\n","# 일부 샘플만 사용 (예: 8개)\n","train_dataset = dataset[\"train\"]\n","random.seed(42)\n","indices = random.sample(range(len(train_dataset)), 8)\n","subset_dataset = Subset(train_dataset, indices)\n","\n","train_dataloader = DataLoader(subset_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n","\n","model_name = \"openai/clip-vit-base-patch32\"\n","model = CLIPModel.from_pretrained(model_name)\n","processor = CLIPProcessor.from_pretrained(model_name)\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","model.eval()\n","\n","embed_dim = model.config.projection_dim\n","\n","# 최대 토큰 길이를 정해야 합니다.\n","# 여기서는 예시로 fixed length(= 77)로 가정. CLIP의 기본 최대 토큰 길이는 일반적으로 77 토큰 정도입니다.\n","max_seq_len = 77\n","w = nn.Parameter(torch.zeros(max_seq_len, embed_dim, dtype=torch.float32))\n","\n","optimizer = optim.Adam([w], lr=1e-3)\n","criterion = nn.CrossEntropyLoss()\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","w = w.to(device)\n","\n","epochs = 50\n","\n","for epoch in range(epochs):\n","    running_loss = 0.0\n","    for images, labels in train_dataloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        texts = [f\"A satellite image showing a {label2class[l].lower()}.\" for l in labels.tolist()]\n","\n","        text_inputs = processor.tokenizer(\n","            texts, padding=True, truncation=True, return_tensors=\"pt\"\n","        ).to(device)\n","\n","        with torch.no_grad():\n","            image_embeds = model.get_image_features(pixel_values=images)\n","            text_outputs = model.text_model(**text_inputs)\n","            # last_hidden_state: [batch, seq_len, embed_dim]\n","            orig_hidden_state = text_outputs.last_hidden_state\n","\n","        # orig_hidden_state.shape: [batch, seq_len, embed_dim]\n","        # w.shape: [seq_len, embed_dim]\n","        # 각 토큰별로 동일한 w를 더하기 위해 broadcasting 사용\n","        # batch 차원: broadcast\n","        # last_hidden_state = orig_hidden_state + w\n","        # 여기서 seq_len은 실제 문장 토큰 길이보다 클 수 있으므로 attention_mask 등을 이용해 실제 토큰 길이에 맞게 적용 가능\n","        # 단순히 앞 부분 토큰만 w 추가한다고 가정\n","        seq_length = orig_hidden_state.size(1)\n","        # 입력된 seq_length가 max_seq_len 이하라고 가정\n","        last_hidden_state = orig_hidden_state + w[:seq_length, :]\n","\n","        # 평균 풀링으로 문장 임베딩 만들기 (예시)\n","        sentence_embeds = last_hidden_state.mean(dim=1)\n","        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n","        sentence_embeds = sentence_embeds / sentence_embeds.norm(p=2, dim=-1, keepdim=True)\n","\n","        logits = torch.matmul(image_embeds, sentence_embeds.t())\n","        target = torch.arange(logits.size(0), device=device)\n","        loss = (criterion(logits, target) + criterion(logits.t(), target)) / 2\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    avg_loss = running_loss / len(train_dataloader)\n","    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n","\n","    # 매 epoch마다 w_sum 계산 및 출력\n","    # w_sum: 각 토큰별로 embed_dim 방향 합\n","    w_sum = w.sum(dim=1)  # [seq_len]\n","    print(\"w_sum vector:\", w_sum.detach().cpu().numpy())\n","    print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OGJ5m5srwwph","executionInfo":{"status":"ok","timestamp":1733804419833,"user_tz":-540,"elapsed":5913,"user":{"displayName":"전현준","userId":"12217885041222534855"}},"outputId":"c0807ecf-a245-4279-e266-2ec23334058f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50, Loss: 1.3847\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 2/50, Loss: 1.3852\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 3/50, Loss: 1.3852\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 4/50, Loss: 1.3844\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 5/50, Loss: 1.3846\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 6/50, Loss: 1.3850\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 7/50, Loss: 1.3857\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 8/50, Loss: 1.3855\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 9/50, Loss: 1.3848\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 10/50, Loss: 1.3856\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 11/50, Loss: 1.3852\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 12/50, Loss: 1.3863\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 13/50, Loss: 1.3847\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 14/50, Loss: 1.3849\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 15/50, Loss: 1.3844\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 16/50, Loss: 1.3850\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 17/50, Loss: 1.3850\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 18/50, Loss: 1.3852\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 19/50, Loss: 1.3857\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 20/50, Loss: 1.3851\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 21/50, Loss: 1.3851\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 22/50, Loss: 1.3851\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 23/50, Loss: 1.3848\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 24/50, Loss: 1.3848\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 25/50, Loss: 1.3844\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 26/50, Loss: 1.3847\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 27/50, Loss: 1.3851\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 28/50, Loss: 1.3846\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 29/50, Loss: 1.3846\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 30/50, Loss: 1.3852\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 31/50, Loss: 1.3856\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 32/50, Loss: 1.3856\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 33/50, Loss: 1.3846\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 34/50, Loss: 1.3847\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 35/50, Loss: 1.3851\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 36/50, Loss: 1.3848\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 37/50, Loss: 1.3844\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 38/50, Loss: 1.3851\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 39/50, Loss: 1.3844\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 40/50, Loss: 1.3855\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 41/50, Loss: 1.3852\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 42/50, Loss: 1.3844\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 43/50, Loss: 1.3844\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 44/50, Loss: 1.3851\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 45/50, Loss: 1.3852\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 46/50, Loss: 1.3844\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 47/50, Loss: 1.3844\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 48/50, Loss: 1.3848\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 49/50, Loss: 1.3858\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","Epoch 50/50, Loss: 1.3857\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Subset\n","from torchvision import transforms\n","from datasets import load_dataset\n","from transformers import CLIPModel, CLIPProcessor\n","import random\n","\n","# EuroSAT 데이터셋 로드\n","dataset = load_dataset('Honaker/eurosat_dataset')\n","\n","label2class = [\n","    \"Annual crop\",\n","    \"Forest\",\n","    \"Herbaceous vegetation\",\n","    \"Highway\",\n","    \"Industrial\",\n","    \"Pasture\",\n","    \"Permanent crop\",\n","    \"Residential\",\n","    \"River\",\n","    \"Sea/Lake\"\n","]\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n","                         (0.26862954, 0.26130258, 0.27577711))\n","])\n","\n","def collate_fn(examples):\n","    images = [transform(x[\"image\"]) for x in examples]\n","    labels = [x[\"label\"] for x in examples]\n","    images = torch.stack(images)\n","    labels = torch.tensor(labels)\n","    return images, labels\n","\n","# 일부 샘플만 사용 (예: 8개)\n","train_dataset = dataset[\"train\"]\n","random.seed(42)\n","indices = random.sample(range(len(train_dataset)), 8)\n","subset_dataset = Subset(train_dataset, indices)\n","\n","train_dataloader = DataLoader(subset_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n","\n","model_name = \"openai/clip-vit-base-patch32\"\n","model = CLIPModel.from_pretrained(model_name)\n","processor = CLIPProcessor.from_pretrained(model_name)\n","\n","# 모델 파라미터 freeze\n","for param in model.parameters():\n","    param.requires_grad = False\n","model.eval()\n","\n","embed_dim = model.config.projection_dim\n","\n","# w를 [max_seq_len, embed_dim] 형태로 정의\n","max_seq_len = 77\n","w = nn.Parameter(torch.zeros(max_seq_len, embed_dim, dtype=torch.float32))\n","\n","optimizer = optim.Adam([w], lr=1e-3)\n","criterion = nn.CrossEntropyLoss()\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","w = w.to(device)\n","\n","epochs = 3\n","\n","for epoch in range(epochs):\n","    running_loss = 0.0\n","    for images, labels in train_dataloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        texts = [f\"A satellite image showing a {label2class[l].lower()}.\" for l in labels.tolist()]\n","\n","        text_inputs = processor.tokenizer(\n","            texts, padding=True, truncation=True, return_tensors=\"pt\"\n","        ).to(device)\n","\n","        # 그래디언트 추적 활성화 (no_grad 제거)\n","        # 모델 파라미터는 requires_grad=False이므로 업데이트 안됨. 그러나 연산은 graph에 포함.\n","        text_outputs = model.text_model(**text_inputs)\n","        orig_hidden_state = text_outputs.last_hidden_state  # [batch, seq_len, embed_dim]\n","\n","        image_embeds = model.get_image_features(pixel_values=images)\n","\n","        seq_length = orig_hidden_state.size(1)\n","        # w[:seq_length, :]를 각 문장에 동일하게 적용\n","        # shape 방송: [batch, seq_len, embed_dim]\n","        last_hidden_state = orig_hidden_state + w[:seq_length, :]\n","\n","        # 문장 임베딩 계산 (단순 평균)\n","        sentence_embeds = last_hidden_state.mean(dim=1)\n","        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n","        sentence_embeds = sentence_embeds / sentence_embeds.norm(p=2, dim=-1, keepdim=True)\n","\n","        logits = torch.matmul(image_embeds, sentence_embeds.t())\n","        target = torch.arange(logits.size(0), device=device)\n","        loss = (criterion(logits, target) + criterion(logits.t(), target)) / 2\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","\n","        # w.grad 확인 (선택적)\n","        print(\"w.grad:\", w.grad)\n","\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    avg_loss = running_loss / len(train_dataloader)\n","    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n","\n","    # 매 epoch마다 w_sum 벡터 출력\n","    w_sum = w.sum(dim=1)  # [seq_len] 형태\n","    print(\"w_sum vector:\", w_sum.detach().cpu().numpy())\n","    print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MYkFSm3XygtR","executionInfo":{"status":"ok","timestamp":1733805382164,"user_tz":-540,"elapsed":2923,"user":{"displayName":"전현준","userId":"12217885041222534855"}},"outputId":"9f41d228-db28-4da4-a303-f4033285daf9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-13-0216b0cf6b22>:109: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n","  print(\"w.grad:\", w.grad)\n"]},{"output_type":"stream","name":"stdout","text":["w.grad: None\n","w.grad: None\n","Epoch 1/3, Loss: 1.3851\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","w.grad: None\n","w.grad: None\n","Epoch 2/3, Loss: 1.3847\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n","w.grad: None\n","w.grad: None\n","Epoch 3/3, Loss: 1.3848\n","w_sum vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Subset\n","from torchvision import transforms\n","from datasets import load_dataset\n","from transformers import CLIPModel, CLIPProcessor\n","import random\n","\n","# EuroSAT 데이터셋 로드\n","dataset = load_dataset('Honaker/eurosat_dataset')\n","\n","label2class = [\n","    \"Annual crop\",\n","    \"Forest\",\n","    \"Herbaceous vegetation\",\n","    \"Highway\",\n","    \"Industrial\",\n","    \"Pasture\",\n","    \"Permanent crop\",\n","    \"Residential\",\n","    \"River\",\n","    \"Sea/Lake\"\n","]\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n","                         (0.26862954, 0.26130258, 0.27577711))\n","])\n","\n","def collate_fn(examples):\n","    images = [transform(x[\"image\"]) for x in examples]\n","    labels = [x[\"label\"] for x in examples]\n","    images = torch.stack(images)\n","    labels = torch.tensor(labels)\n","    return images, labels\n","\n","# 일부 샘플만 사용 (예: 8개)\n","train_dataset = dataset[\"train\"]\n","random.seed(42)\n","indices = random.sample(range(len(train_dataset)), 8)\n","subset_dataset = Subset(train_dataset, indices)\n","\n","train_dataloader = DataLoader(subset_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n","\n","model_name = \"openai/clip-vit-base-patch32\"\n","model = CLIPModel.from_pretrained(model_name)\n","processor = CLIPProcessor.from_pretrained(model_name)\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","model.eval()\n","\n","embed_dim = model.config.projection_dim\n","\n","# 최대 토큰 길이\n","max_seq_len = 77\n","w = nn.Parameter(torch.zeros(max_seq_len, embed_dim, dtype=torch.float32))\n","\n","optimizer = optim.Adam([w], lr=1e-3)\n","criterion = nn.CrossEntropyLoss()\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","w = w.to(device)\n","\n","epochs = 50\n","\n","for epoch in range(epochs):\n","    running_loss = 0.0\n","    for images, labels in train_dataloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        texts = [f\"A satellite image showing a {label2class[l].lower()}.\" for l in labels.tolist()]\n","        text_inputs = processor.tokenizer(\n","            texts, padding=True, truncation=True, return_tensors=\"pt\"\n","        ).to(device)\n","\n","        with torch.no_grad():\n","            image_embeds = model.get_image_features(pixel_values=images)\n","\n","        # 토큰 임베딩 및 포지션 임베딩 직접 계산\n","        input_ids = text_inputs.input_ids\n","        attention_mask = text_inputs.attention_mask\n","        input_shape = input_ids.size()  # [batch_size, seq_length]\n","        batch_size, seq_length = input_shape\n","\n","        with torch.no_grad():\n","            inputs_embeds = model.text_model.embeddings.token_embedding(input_ids)\n","            position_ids = torch.arange(seq_length, dtype=torch.long, device=device).unsqueeze(0).expand(batch_size, -1)\n","            position_embeddings = model.text_model.embeddings.position_embedding(position_ids)\n","            hidden_states = inputs_embeds + position_embeddings\n","            # dropout 제거됨\n","\n","        # w 추가\n","        hidden_states = hidden_states + w[:seq_length, :]\n","\n","        # 인코더 통과\n","        encoder_outputs = model.text_model.encoder(\n","            hidden_states,\n","            attention_mask=attention_mask,\n","            output_attentions=False,\n","            output_hidden_states=False,\n","            return_dict=True,\n","        )\n","\n","        last_hidden_state = encoder_outputs.last_hidden_state\n","        # final_layer_norm 적용\n","        last_hidden_state = model.text_model.final_layer_norm(last_hidden_state)\n","\n","        # 평균 풀링으로 문장 임베딩\n","        sentence_embeds = last_hidden_state.mean(dim=1)\n","        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n","        sentence_embeds = sentence_embeds / sentence_embeds.norm(p=2, dim=-1, keepdim=True)\n","\n","        logits = torch.matmul(image_embeds, sentence_embeds.t())\n","        target = torch.arange(logits.size(0), device=device)\n","        loss = (criterion(logits, target) + criterion(logits.t(), target)) / 2\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    avg_loss = running_loss / len(train_dataloader)\n","    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n","\n","    w_sum = w.sum(dim=1).detach().cpu().numpy()\n","    print(\"w_sum vector:\", w_sum)\n","    print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"dGD2Edej1e6X","executionInfo":{"status":"error","timestamp":1733826568120,"user_tz":-540,"elapsed":2784,"user":{"displayName":"전현준","userId":"12217885041222534855"}},"outputId":"c0b57c8c-5cd0-4087-f8d6-89e6920a45b2"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Expected attn_mask dtype to be bool or float or to match query dtype, but got attn_mask.dtype: long int and  query.dtype: float instead.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-c98e7ea1a4b2>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# 인코더 통과\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         encoder_outputs = model.text_model.encoder(\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    875\u001b[0m                 )\n\u001b[1;32m    876\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m                 layer_outputs = encoder_layer(\n\u001b[0m\u001b[1;32m    878\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         hidden_states, attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    609\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;31m# CLIP text model uses both `causal_attention_mask` and `attention_mask` sequentially.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m    541\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected attn_mask dtype to be bool or float or to match query dtype, but got attn_mask.dtype: long int and  query.dtype: float instead."]}]},{"cell_type":"code","source":[],"metadata":{"id":"5zAriQCRGvK9"},"execution_count":null,"outputs":[]}]}